{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to vector unit  length / unit norm\n",
    "\n",
    "Scaling to unit norm is achieved by dividing each feature vector by either the Manhattan distance (l1 norm) or the Euclidean distance of the vector (l2 norm):\n",
    "\n",
    "X_scaled_l1 = X / l1(X)\n",
    "\n",
    "X_scaled_l2 = X / l2(X)\n",
    "\n",
    "\n",
    "The **Manhattan distance** is given by the sum of the absolute components of the vector:\n",
    "\n",
    "l1(X) = |x1| + |x2| + ... + |xn|\n",
    "\n",
    "\n",
    "Whereas the **Euclidean distance** is given by the square root of the square sum of the component of the vector:\n",
    "\n",
    "l2(X) = sqr( x1^2 + x2^2 + ... + xn^2 )\n",
    "\n",
    "\n",
    "In the above example, x1 is variable 1, x2 variable 2, and xn variable n, and X is the data for 1 observation across variables (a row in other words).\n",
    "\n",
    "\n",
    "### Scaling to unit norm, examples\n",
    "\n",
    "For example, if our data has 1 observations (1 row) and 3 variables:\n",
    "\n",
    "- number of pets\n",
    "- number of children\n",
    "- age\n",
    "\n",
    "The values for each variable for that single observation are 10, 15 and 20. Our vector X = [10, 15, 20]. Then:\n",
    "\n",
    "l1(X) = 10 + 15 + 20 = 45\n",
    "\n",
    "l2(X) = sqr( 10^2 + 15^2 + 20^2) = sqr( 100 + 225 + 400) = **26.9**\n",
    "\n",
    "The euclidean distance is always smaller than the Manhattan distance.\n",
    "\n",
    "\n",
    "The normalized vector values are therefore:\n",
    "\n",
    "X_scaled_l1 = [ 10/45, 15/45, 20/45 ]      =  [0.22, 0.33, 0.44]\n",
    "\n",
    "X_scaled_l2 = [10/26.9, 15/26.9, 20/26.9 ] =  [0.37, 0.55, 0.74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataset for the demo\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the scaler - for robust scaling\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the the Boston House price data\n",
    "\n",
    "# this is how we load the boston dataset from sklearn\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# create a dataframe with the independent variables\n",
    "data = pd.DataFrame(boston_dataset.data,\n",
    "                    columns=boston_dataset.feature_names)\n",
    "\n",
    "# add target\n",
    "data['MEDV'] = boston_dataset.target\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (152, 13))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l1') # for euclidean distance we change to norm='l2' \n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1024.1,  744. , 1003.5,  858. ,  732.4,  859.3,  845.9,  847. ,\n",
       "        888.1,  826.7,  705.9,  808.9,  734. ,  784.6,  785.1,  819.6,\n",
       "       1212.9, 1258. ,  757.6, 1223. , 1236.1,  890.2, 1166.8, 1270. ,\n",
       "        919. ,  925.9, 1197. ,  770.3, 1118. ,  886.7,  719. ,  952. ,\n",
       "        933.4,  745. ,  805.6,  825.1,  678.5,  947.7,  910. ,  816. ,\n",
       "        769.1,  832.4,  763.5,  777. ,  850.4, 1137.7,  802.1,  703.4,\n",
       "        802.6,  688. ,  801.8,  877.3,  753.8,  912.4,  936.7,  813.6,\n",
       "       1194. ,  771.9,  739.5,  825.9,  890.8,  891.6,  844.4,  927.1,\n",
       "        998.6,  903.3, 1250.3,  739.5, 1095.3,  857.7,  834.8,  738.4,\n",
       "        855.1, 1238.4,  828.6,  958.4,  775. ,  923.4,  859.6,  830.7,\n",
       "        782.3,  796.8,  953.2,  857.9, 1249.9,  716.7,  803.9,  978.8,\n",
       "       1186.9,  904. , 1238.2,  981.6,  884.9,  924.9,  694.1,  725.2,\n",
       "        832.4,  717.7,  917. , 1263.9,  836.7,  813.5,  992.5,  823.4,\n",
       "       1224.3,  712.1,  841.2,  950.3,  702.6,  791.9,  830.4,  742.1,\n",
       "        793.8, 1110.5,  832.1, 1188.5,  837.4,  846.7,  809.5,  764.6,\n",
       "       1189. ,  770.4,  651.4,  717.8, 1265.8, 1004. ,  782.1,  922.7,\n",
       "        871.4,  860. ,  860.1,  766.5, 1259.6,  920.1,  708.2,  779.3,\n",
       "        785.7,  815. ,  825. ,  856.5,  784.2,  834.9, 1003.2,  963.2,\n",
       "        924.1, 1141.8, 1192.2,  777.5,  767.7, 1197.4,  887.2,  839.3,\n",
       "        819.9,  818.9,  822.7, 1006.2,  926.2,  748.9,  920.8,  790.7,\n",
       "        810.2, 1235.9,  889. ,  921.2,  895.3,  774.9,  924.8,  926. ,\n",
       "       1248.6, 1006.3,  817.5,  800.7, 1227.4,  831.4,  787.4, 1312. ,\n",
       "        832.1,  769.5, 1252.1,  822.7,  884. ,  787.2,  749.4,  827.8,\n",
       "        962.2,  702.8,  745.4,  731.7,  727.8,  743.7,  888.3,  720.2,\n",
       "       1332.3, 1185.2,  776.4,  784.6, 1255.2,  717.8, 1239. ,  782.1,\n",
       "        767.9,  807.6,  857.5, 1265.4,  737.9,  800. ,  769.3, 1231.7,\n",
       "        783.1,  667. ,  726.4,  758.5,  841.3, 1243.1, 1268.4, 1174.9,\n",
       "        736. ,  828.9,  827.5, 1270. , 1000.2,  919.4,  866. ,  681.7,\n",
       "       1249. ,  810.9,  901.7, 1218.9,  928.7,  744.7,  843.4,  736.1,\n",
       "        897.6,  804.6, 1219.5,  794.5,  830.4, 1241.1,  732. , 1211.2,\n",
       "        890.6,  838.1,  753.5,  795.8,  801.3, 1277.8, 1256. ,  801.3,\n",
       "        704.7,  808.2,  931.3,  977.4,  702.4,  713.7, 1240.1, 1001.4,\n",
       "        867.2,  806. ,  802.1,  774.1,  783.6,  739.2,  719.6, 1276.7,\n",
       "       1264.7,  865.4, 1104.8, 1162.6,  778.9,  781.8,  776.8,  945.3,\n",
       "       1234.4,  794.1,  830.3,  880.1,  788.6,  799.5, 1231.8, 1236.9,\n",
       "        669.1,  934.3, 1253.3, 1239.6,  734.5,  855.2,  753.4, 1260.9,\n",
       "        730.5, 1182.1, 1253. , 1004.4,  793.7,  847.5,  724.4, 1237.2,\n",
       "        941.6,  906.5,  859.6,  877.7,  842. ,  694.3,  997.8,  695.3,\n",
       "       1251.1, 1167. ,  837.6,  855. , 1301.8, 1238.6,  968.8,  822.6,\n",
       "       1216.7,  807.1,  775.9,  707.8, 1260.5,  943.2,  706.4,  839.5,\n",
       "        759.2, 1221.5,  923.8, 1219.9,  780.6,  755.1,  701.2,  757.2,\n",
       "        797.2,  731.4, 1237.1,  802.4, 1265.4,  785.5,  766.3,  839.7,\n",
       "       1255.2,  792.9, 1224.5,  748.3,  743.8,  790.3,  808.3,  760.4,\n",
       "        828.9,  854.6, 1226.7,  797. ,  788. ,  807.4,  900.5,  962.7,\n",
       "        770.2,  830.6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the norm for each observation (feature vector)\n",
    "# original data\n",
    "\n",
    "np.round( np.linalg.norm(X_train, ord=1, axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the norm for each observation (feature vector)\n",
    "# scaled data\n",
    "\n",
    "np.round( np.linalg.norm(X_train_scaled, ord=1, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the norm for each observation (feature vector)\n",
    "# scaled data\n",
    "\n",
    "np.round( np.linalg.norm(X_train_scaled, ord=2, axis=1), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feml",
   "language": "python",
   "name": "feml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
